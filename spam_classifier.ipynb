{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador de spam usando python\n",
    "\n",
    "Este notebook mostrará frutos dos meus estudos sobre NLP usando python. Nele combinaremos técnicas de Machine Learning, processamento de texto e estatística para obter os textos em um formato que o algoritmo de aprendizado usado possa entender. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendendo os dados\n",
    "\n",
    "O conjunto de dados desse notebook pertence a UCI. O arquivo tem mais de 5 mil mensagens rotuladas como HAM ou SPAM. \n",
    "\n",
    "Primeiramente iremos obter todas as mensagens em uma variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [line.rstrip() for line in open('smsspamcollection/SMSSpamCollection')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O arquivo é separado por tabulação\n",
    "\n",
    "Usaremos o pandas para não ter que analisar os arquivos manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tipo</th>\n",
       "      <th>mensagem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tipo                                           mensagem\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',\n",
    "                           names=[\"tipo\", \"mensagem\"])\n",
    "mensagens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando o texto para o modelo\n",
    "\n",
    "O principal problema a ser enfrentado aqui é que os dados são do tipo String e o algoritmo que será utilizado necessita de um vetor numérico para fazer a classificação. \n",
    "\n",
    "Um dos métodos mais simples para converter o texto em um vetor é o Bag of Words, sua tradução livre seria \"Saco de palavras\".  Nele cada palavra única é representada por um número. \n",
    "\n",
    "O primeiro passo para preparar o texto é remover as pontuações e as chamadas stopwords de cada mensagem do nosso dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[0:10] # Alguns exemplos de stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isso, criaremos a seguinte função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessamento(msg):\n",
    "    # Retira pontuações\n",
    "    noPont = [char for char in msg if char not in string.punctuation]\n",
    "\n",
    "    # Junta-os para formar strings\n",
    "    noPont = ''.join(noPont)\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    return [word for word in noPont.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: mensagem, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagens['mensagem'].head(5).apply(preProcessamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar que nossa função funciona e retorna apenas as palavras que realmente nos importam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando as mensagens em um vetor\n",
    "\n",
    "Vamos fazer isso em três etapas usando o modelo Bag of Words:\n",
    "\n",
    "    1. Contar quantas vezes ocorre uma palavra em cada mensagem (conhecida como frequência de termo)\n",
    "\n",
    "    2. Pesar as contagens, de modo que as palavras frequentes recebem menor peso (frequência inversa do documento)\n",
    "\n",
    "    3. Normalize os vetores para o comprimento da unidade, para abstrair do comprimento do texto original (norma L2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível notar que cada vetor terá **n** dimensões onde **n** = **quantidade de palavras únicas na mensagem**. \n",
    "\n",
    "Usaremos o CountVectorizer do SciKit Learn. Este modelo converterá uma coleção de documentos de texto em uma matriz de contagem de palavras relevantes no nosso texto. \n",
    "\n",
    "O resultado será uma matriz bidimensional onde as colunas são representadas pelas mensagens e as linhas a contagem de palavras. \n",
    "\n",
    "Por exemplo: \n",
    "\n",
    "![title](table.png)\t\t\t\n",
    "\n",
    "Há muitos argumentos e parâmetros que podem ser passados para o CountVectorizer. Neste caso, vamos especificar o analyzer para ser nossa própria função previamente definida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "# Isso pode demorar um pouco\n",
    "bow_transformer = CountVectorizer(analyzer=preProcessamento).fit(mensagens['mensagem'])\n",
    "\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora iremos usar nosso objeto 'bow_transformer' para transformar todo o dataframe de mensagens em suas representações vetoriais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(mensagens['mensagem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a contagem, o termo ponderação e normalização pode ser feito com TF-IDF, usando o TfidfTransformer do scikit-learn.\n",
    "\n",
    "### TF - IDF\n",
    "\n",
    "O valor TF-IDF, é uma medida estatística que tem o intuito de indicar a importância de uma palavra de um documento em relação a uma coleção de documentos ou em um corpus linguístico. Ela é frequentemente utilizada como fator de ponderação na recuperação de informações e na mineração de dados.\n",
    "\n",
    "O valor TF-IDF de uma palavra aumenta proporcionalmente à medida que aumenta o número de ocorrências dela em um documento, no entanto, esse valor é equilibrado pela frequência da palavra no corpus. Isso auxilia a distinguir o fato da ocorrência de algumas palavras serem geralmente mais comuns que outras. \n",
    "\n",
    "Normalmente, o peso de TF-IDF é composto por dois termos: o primeiro calcula a Freqüência do termo normalizada (TF), ou seja, o número de vezes que uma palavra aparece em um documento, dividido pelo número total de palavras nesse documento; O segundo termo é a Freqüência do Documento Inverso (IDF), calculado como o logaritmo do número de documentos no corpus dividido pela quantidade de documentos onde o termo específico aparece.\n",
    "\n",
    "TF(x) = número de vezes que X aparece em um documento / número total de termos do documento\n",
    "\n",
    "IDF(x) = log_e (Número total de documentos / Número de documentos com termo X nele)\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Seja B um conjunto com 1000 documentos e A um documento que contém 100 palavras. Supondo que a palavra \"java\" aparece 3 vezes no documento A e que em 10 dos 1000 documentos aparece a palavra \"java\", temos: \n",
    "\n",
    "O TF(x) para x = \"java\" é igual a 3/100. Isto é 0,03.\n",
    "\n",
    "O IDF(x) do documento é igual log(1000/10). Isto é 2.\n",
    "\n",
    "\n",
    "Agora faremos isso no SciKit Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o modelo e fazendo predições\n",
    "\n",
    "Com mensagens representadas como vetores, podemos finalmente treinar nosso classificador de spam. Agora, podemos realmente usar quase qualquer tipo de algoritmos de classificação. Para esse modelo o algoritmo do classificador Naive Bayes é uma boa escolha.\n",
    "\n",
    "Primeiramente iremos dividir nossos dados em dados de treino e dados de teste.\n",
    "\n",
    "Vamos executar o nosso modelo novamente e depois prever o conjunto de testes. Usaremos os recursos pipeline do SciKit Learn para armazenar uma linha de fluxo de trabalho. Isso nos permitirá configurar todas as transformações que faremos aos dados para uso futuro. Vejamos um exemplo de como funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457 1115 5572\n"
     ]
    }
   ],
   "source": [
    "msg_train, msg_test, tipo_train, tipo_test = \\\n",
    "train_test_split(mensagens['mensagem'], mensagens['tipo'], test_size=0.2)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=preProcessamento)),  # Tokeniza as mensagens\n",
    "    ('tfidf', TfidfTransformer()),  # Faz a transformação em TF-IDF\n",
    "    ('classifier', MultinomialNB()),  # Define a classe que realizará nossa classificação.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function preProcessamento at 0x7f1ff6387440>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(msg_train,tipo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.96      0.98      1013\n",
      "        spam       0.71      1.00      0.83       102\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.85      0.98      0.90      1115\n",
      "weighted avg       0.97      0.96      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions,tipo_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos inferir do que foi printado que tivemos um excelente modelo e um bom resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
